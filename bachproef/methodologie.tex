%%=============================================================================
%% Methodologie
%%=============================================================================

\chapter{Methodologie}
\label{ch:methodologie}
\lstset{
    tabsize = 4, %% Sets tab space width.
    showstringspaces = false, %% Prevents space marking in strings, string is defined as the text that is generally printed directly to the console.
    numbers = left, %% Displays line numbers on the left.
    commentstyle = \color{green}, %% Sets comment color.
    keywordstyle = \color{blue}, %% Sets  keyword color.
    stringstyle = \color{red}, %% Sets  string color.
    rulecolor = \color{black}, %% Sets frame color to avoid being affected by text color.
    basicstyle = \small \ttfamily , %% Sets listing font and size.
    breaklines = true, %% Enables line breaking.
    numberstyle = \tiny,
}

%%\begin{lstlisting}[language = Java , frame = trBL , firstnumber = last , escapeinside={(*@}{@*)}]
%%public class Factorial
%%{
%%public static void main(String[] args)
%%{   final int NUM_FACTS = 100;
%%for(int i = 0; i < NUM_FACTS; i++)
%%System.out.println( i + "! is " + factorial(i));
%%}
%%
%%public static int factorial(int n)
%%{   int result = 1;
%%for(int i = 2; i <= n; i++) (*@\label{for}@*)
%%result *= i;
%%return result;
%%}
%%}
%%\end{lstlisting}


%% TODO: Hoe ben je te werk gegaan? Verdeel je onderzoek in grote fasen, en
%% licht in elke fase toe welke stappen je gevolgd hebt. Verantwoord waarom je
%% op deze manier te werk gegaan bent. Je moet kunnen aantonen dat je de best
%% mogelijke manier toegepast hebt om een antwoord te vinden op de
%% onderzoeksvraag.

% TODO: uitleg wat we allemaal nodig hebben, producer uitleggen, consumer uitleggen, 3 verschillende groottes uitleggen.
Om te testen welke van de technologieën die in dit onderzoek aan bod komen nu eigenlijk het beste is, zou je met veel zaken rekening moeten houden. Wat het beste is hangt vooral af van welke soort data je gebruikt. Ook de hoeveelheid transformaties die je data ondergaat voor dat ze uitgelezen worden of verzonden speelt een grote rol. Natuurlijk is het moeilijk om binnen de tijdspanne die er is voor dit onderzoek al deze factoren te gaan onderzoeken. Daarom zal dit onderzoek zich toespitsen op een bepaald scenario en daar conclusies uit trekken. Voor het scenario beslissen we op voorhand voor welk formaat van de data dat we dit onderzoek zullen voeren, dit zal een Json zijn. Ook zullen we het onderzoek drie keer uitvoeren, voor verschillende groottes van data. Een keer met 10 000, 100 000 en 1 000 000.  

Voor dit onderzoek zijn er twee grote applicaties nodig. Een producer en een consumer applicatie. De producer zorgt ervoor dat er per technologie een implementatie is waardoor een technologie data kan verzenden. In dit onderzoek zal dit voor Kafka, RabbitMq en Google Pub/Sub gebeuren. Elke keer deze applicatie opgestart wordt, zal er meegegeven worden voor welke technologie er data moet verzonden worden.

Voor de tweede applicatie is er ook een implementatie nodig per technologie die ervoor zal zorgen dat er geluisterd wordt naar de topic/queue indien er nieuwe messages zijn. Deze verschillende consumers zullen tegelijkertijd luisteren naar hun eigen topic/queue. Deze applicatie moet dus niet apart opgestart worden.

Nu gaan we de implementatie overlopen van objecten van klassen die onafhankelijk zijn van een technologie.

\section{Producer project}
Natuurlijk moet er iets gelijkaardigs zijn om te kunnen vergelijken. Het data object dat we zullen verzenden en ontvangen bij zowel \emph{Kafka}, \emph{RabbitMq} en \emph{Google Pub/Sub} zal hetzelfde zijn. Op deze manier is het toekomstige resultaat niet afhankelijk van het type data. Dit onderzoek zal zich dus toespitsen op data met het type Json. Deze klasse (Data.class) hergebruiken we in elke technologie. Deze klasse stelt het type voor van het bericht dat we verzenden.
\begin{lstlisting}[language = Java , frame = trBL , firstnumber = last , escapeinside={(*@}{@*)}]
@Getter
@AllArgsConstructor
@NoArgsConstructor
@ToString
public class Data {
    private int id;
    private String name;
    private String description;
    private Date sendedDate;
}
\end{lstlisting}

Deze klasse bevat vier attributen. Dit is om een Json object na te bootsen. Het is vanzelfsprekend dat deze klasse niet zo maar automatisch kan omgezet worden naar een Json. De manier waarop dit gedaan wordt is opnieuw per technologie verschillend. De uitleg zal gegeven worden in de subsectie van de technologie zelf. De attributen die hier gebruikt worden zijn: een id van het type int, dit is om elk data object van elkaar te kunnen onderscheiden tijdens het verzenden. Als tweede zie je name van het type String. Dit heeft als bedoeling opnieuw om verschillende objecten van elkaar te kunnen onderscheiden en om een realistisch attribuut te geven aan het Data-object. Deze doelstelling is dezelfde voor het attribuut description van het type String. Als laatste hebben we de sendedDate van het type Date. Deze zal de datum en het tijdstip bijhouden wanneer het object verzonden is. Dit zal later nog eens aangehaald worden wanneer we deze objecten genereren. 

Als u boven de klasse kijkt dan ziet u nog vier annotaties staan. Deze kunnen we gebruiken door de library van Lombok. Dit werd mogelijk gemaakt door deze dependency toe te voegen aan onze pom.xml want het gebruikte project voor dit onderzoek maakt gebruik van Maven om libraries toe te voegen.

\begin{lstlisting}[language = xml , frame = trBL , firstnumber = 1 , escapeinside={(*@}{@*)}]
<dependency>
    <groupId>org.projectlombok</groupId>
    <artifactId>lombok</artifactId>
    <optional>true</optional>
</dependency>
\end{lstlisting}

De library Lombok heeft op voorhand al enkele handige implementaties. De gebruikte annotaties zullen we uitleggen, want natuurlijk bestaan er meer dan enkel deze die dit onderzoek gebruikt. Voor de andere mogelijkheden verwijs ik u graag door naar de site van Project Lombok waar u alle mogelijkheden op een rijtje ziet: https://projectlombok.org/features/all. 

De eerste annotatie is @Getter, deze zorgt ervoor zoals de naam al doet vermoeden, dat er voor ieder attribuut een getter aanwezig is. Op deze manier staat uw code niet overvol met getters van al uw attributen. In deze klasse zijn er niet zo veel attributen dus zou het aantal getter nog meevallen. Maar u kan zich wel inbeelden dat in een groter project in een klasse met veel attributen en andere methodes dat dit een handige annotatie is. Je ziet dus niet dat de getters aanwezig zijn maar die zijn er wel effectief. Laten we de tweede en de derde samen bekijken: @AllArgsConstructor en @NoArgsConstructor. Deze zorgen ervoor dat er een constructor gegenereerd wordt met respectievelijk al de attributen en zonder de attributen. Dit zijn vaak voorkomende constructors en het is handig om ook hiervoor een annotatie te hebben. De laatste  is de @ToString. Deze werd gebruikt tijdens het testen van het project. Dit zorgt voor een String-representatie van het object. Dit was handig om lokaal te testen. Het data object werd dan uitgeprint tijdens het verzenden om te controleren of het object wel de juiste waardes bevatte.

\subsection{Creëren van de data}
De implementatie van de verschillende technologieën komt in het volgende hoofdstuk aan bod. Het enige werk dat in dit hoofdstuk voor de Producer applicatie overblijft, is het creëren van de data en het verzenden ervan. Er zijn nog drie cruciale elementen die besproken moeten worden en die zijn: de ProducerApplication- , de RandomDataProvider- en de RandomDataPublisher.

\subsubsection{ProducerApplication}
Dit is de eerste klasse die gebruikt wordt om de applicatie te starten. In de methode runner geven we via de parameters alle klassen mee die data publishen. Dit is dus een object van de klasse van GooglePubSubPublisher, KafkaPublisher en RabbitMqPublisher. De implementatie van deze klassen worden ook in volgend hoofdstuk besproken. Ook moeten we de klasse RandomDataPublisher meegeven. In deze methode roep je de methode doStuff op van de RandomDataPublisher klasse. Wat deze methode exact doet bespreken we later. Maar je moet meegeven welke publisher je wilt gebruiken. Deze lijn code is het enige dat je moet aanpassen om data te verzenden voor één van de drie technologieën.

\begin{lstlisting}[language = Java , frame = trBL , firstnumber = 1 , escapeinside={(*@}{@*)}]
@SpringBootApplication
@Slf4j
public class ProducerApplication {

    public static void main(String[] args) {
        SpringApplication.run(ProducerApplication.class, args);
    }

    @Bean
    public CommandLineRunner runner(final RandomDataPublisher randomDataPublisher,
        final GooglePubSubPublisher googlePubSubPublisher,
        final KafkaPublisher kafkaPublisher,
        final RabbitMqPublisher rabbitMqPublisher) {
            return args -> {
                       randomDataPublisher.doStuff(googlePubSubPublisher);
    //   OF:         randomDataPublisher.doStuff(kafkaPublisher);
    //   OF;         randomDataPublisher.doStuff(rabbitMqPublisher);
                    };
      }
}
\end{lstlisting}
\subsubsection{RandomDataProvider}
Deze klasse is een hulpklasse die de lijst van Data objecten genereert. Het aantal objecten wordt meegegeven als parameter en wordt ingesteld in de RandomDataPublisher. De naam van het object en de beschrijving is telkens de naam van het attribuut, respectievelijk name en description, plus het id eraan geplakt. Hier wordt ook een nieuwe datum gecreëerd die dient als sendedDate.
\begin{lstlisting}[language = Java , frame = trBL , firstnumber = 1 , escapeinside={(*@}{@*)}]
@Component
public class RandomDataProvider {
    public List<Data> create(int numberOfEntries) {
        final List<Data> dataList = new ArrayList<>();
        for (int i = 0; i < numberOfEntries; i++) {
            dataList.add(new Data(i, "name" + i, "description" + i, new Date()));
        }

        return dataList;
    }
}
\end{lstlisting}

\subsubsection{RandomDataPublisher}
Deze klasse heeft de functionaliteit om de methode create uit de klasse RandomDataProvider op te roepen om de Data objecten te genereren en door te geven aan de juiste publisher. De constructor heeft opnieuw de publisher klassen als parameter die gecreëerd worden door Spring Boot. Het heeft ook nog een extra parameter namelijk een instantie van de RandomDataProvider om later de create methode te kunnen gebruiken.

We verdiepen ons even in de methode doStuff. Hierin wordt aan de randomDataProvider gevraagd om een aantal Data objecten te maken. In dit codevoorbeeld is dit 10000 objecten. Deze slaat hij op in een lijst van het type Data. In de methode wordt er gecontroleerd van welke instantie de publisher is die je mee krijgt. Indien de publisher van Kafka of RabbitMq is dan wordt deze lijst verzonden naar de Kafka topic of de RabbitMq exchange. Is het een instantie van de publisher van Google Pub/Sub dan wordt deze lijst overlopen en per object verstuurd. De publish methode van GooglePubSubPublisher wenst maar één object en geen lijst van objecten.

\begin{lstlisting}[language = Java , frame = trBL , firstnumber = 1 , escapeinside={(*@}{@*)}]
@Component
public class RandomDataPublisher {

    private final RandomDataProvider randomDataProvider;
    private final KafkaPublisher kafkaPublisher;
    private final GooglePubSubPublisher googlePubSubPublisher;
    private final RabbitMqPublisher rabbitMqPublisher;

    public RandomDataPublisher(RandomDataProvider randomDataProvider,
        KafkaPublisher kafkaPublisher,
        GooglePubSubPublisher googlePubSubPublisher,
        RabbitMqPublisher rabbitMqPublisher) {
            this.randomDataProvider = randomDataProvider;
            this.kafkaPublisher = kafkaPublisher;
            this.googlePubSubPublisher = googlePubSubPublisher;
            this.rabbitMqPublisher = rabbitMqPublisher;
    }

    public void doStuff(Object object) {
            final List<Data> dataList = randomDataProvider.create(10_000);
            if(object instanceof GooglePubSubPublisher) {
                for (Data data : dataList) {
                    googlePubSubPublisher.publishData(data);
                }
            } else if (object instanceof KafkaPublisher){
                kafkaPublisher.publish(dataList);
            } else {
                rabbitMqPublisher.publish(dataList);
            }
    }
}
\end{lstlisting}
\section{Consumer project}
De bedoeling van deze applicatie is dat ze wanneer er iets op een topic of queue geplaatst wordt, ze deze informatie weer omzet en opslaat in een lokale databank. Bij het opslaan van de data wordt er meer opgeslagen dan dat er informatie binnenkomt om deze te kunnen vergelijken, maar dit wordt straks duidelijker. Wanneer deze applicatie opgestart is, luistert deze voortdurend naar de topics, of naar de queue, of er messages op geplaatst worden. Indien er een message op een topic of de queue toegekomen is, dan leest hij deze meteen uit. Deze applicatie luistert dus tegelijkertijd naar zowel de topic van Kafka en Google Pub/Sub als naar de queue van RabbitMq. Om dit niet met elkaar te laten beïnvloeden wordt tijdens dit onderzoek enkel data verstuurd van een bepaalde technologie, bijvoorbeeld Kafka, indien de consumerapplicatie niets meer leest van een andere technologie. Deze aanpak zorgt er dus voor dat data van verschillende technologieën niet door elkaar gelezen worden. We hebben in deze applicatie hetzelfde domein object van de klasse Data, want we willen het object dat verstuurd werd op dezelfde manier terugkrijgen. Door later de juiste deserializers mee te geven, moeten we ons geen zorgen maken en wordt het inkomende object automatisch gemapt naar hetzelfde domeinobject.
\begin{lstlisting}[language = Java , frame = trBL , firstnumber = 1 , escapeinside={(*@}{@*)}]
@Getter
@AllArgsConstructor
@NoArgsConstructor
@ToString
public class Data {
    private int id;
    private String name;
    private String description;
    private Date sendedDate;
}
\end{lstlisting}

Naast deze Data klasse hebben we ook per technologie een entity klasse en repository. De entity klasse zorgt ervoor dat per technologie een tabel gecreëerd wordt in de databank. De repository zorgt ervoor dat een object van deze klasse effectief opgeslagen kan worden. De namen van deze entity klassen zijn: DataGPS, DataKafka en DataRMQ. Voor de repository zijn de namen van de interfaces: DataGPSRepository, DataKafkaRepository en DataRMQRepository. De implementatie van deze klassen is telkens dezelfde, maar het is nodig om deze apart te houden verschillende tabellen te hebben in de databank per technologie. In het volgende hoofdstuk zullen we één entity klasse en repository uitleggen want voor de andere is het analoog.
\section{Onderzoek}
Zoals al eerder vermeld worden de resultaten opgeslagen in een databank. De databank draait lokaal in een Docker container. Dit is de eenvoudige configuratie die nodig is om te databank op te stellen:
\begin{lstlisting}[language = xml , frame = trBL , firstnumber = 1 , escapeinside={(*@}{@*)}]
spring:
    datasource:
        url: "jdbc:postgresql://localhost:5432/postgres"
        username: postgres
        password: postgres
    jpa:
        hibernate:
            ddl-auto: create
        jdbc:
            lob:
                non-contextual-creation: true
\end{lstlisting}
De URL van de databank wordt meegegeven met daarachter de naam van de databank. Daaronder geef je de gebruikersnaam en wachtwoord mee. Het is niet erg dat het wachtwoord bloot gegeven wordt, want dit is toch een lokale databank. Verder zie je ook dat indien de applicatie opnieuw opstart, de database opnieuw gecreëerd wordt.

Verder werd dit onderzoek drie keer uitgevoerd. Een keer met 10000, 100000 en 1000000 objecten van de klasse Data die verzonden werden. Zo kunnen we ook vergelijken hoe sterk het tijdsverschil wordt. De bedoeling was om de methodes die de objecten ontvangen zo klein mogelijk te houden. Want hoe groter deze methodes worden, hoe langer het duurt per object om deze objecten te ontvangen en dit zou een verkeerd beeld kunnen geven.

De resultaten werden per technologie opgeslagen in een CSV-file om daarna berekeningen op te kunnen doen. Dit werd gedaan met een Python script. 

Eerst wordt de CSV uitgelezen via de pandas library. Hierna wordt er wat aan data exploratie gedaan om te controleren of de waardes die uitgelezen werden wel effectief mogelijk zijn. 
\begin{lstlisting}[language = python , frame = trBL , firstnumber = 1 , escapeinside={(*@}{@*)}]
import pandas as pd
result = pd.read_csv('/Users/rubendesmet/Desktop/gps10_000.csv',sep=',')
print(result.head())
print(result.columns)
print(result.index)
print(result.count())
print(result.shape)
\end{lstlisting}

Vervolgens worden de kolommen die niet van toepassing zijn verwijderd. Enkel de kolom timeDiff zou moeten overblijven die het verschil van de ontvangen en verzonden tijd voorstelt. Daarna kijken we hoeveel rijen er in de tabel zitten, worden eventuele NaN verwijderd, en kijken we nog eens hoeveel rijen er in de tabel zitten. Als laatste controlemiddel printen we nog eens de eerste vijf rijen af en zo kunnen we ook zien of we kolommen vergeten verwijderen zijn. 

\begin{lstlisting}[language = python , frame = trBL , firstnumber = last , escapeinside={(*@}{@*)}]
print(result.count())
result = result.dropna()
print(result.count())
print(result.head())
\end{lstlisting}

Dan rest ons nog één ding, namelijk het berekenen van het gemiddelde van deze kolom, samen met het minimum en maximum.
\begin{lstlisting}[language = python , frame = trBL , firstnumber = last , escapeinside={(*@}{@*)}]
print(result['timeDiff'].mean())
print(result['timeDiff'].max())
print(result['timeDiff'].min())
\end{lstlisting}
Al deze stappen worden dus negen keer uitgevoerd. Drie keer per technologie en drie keer per hoeveelheid verzonden data.

\subsubsection{Tweede deel van het onderzoek}
Nadat dit allemaal onderzocht was, werd er beslist door het bedrijf TVH dat we ook nog het gebruikte geheugen zouden onderzoeken. Hiervoor moest er niets aangepast worden aan de producer applicatie, maar wel aan de consumer applicatie. 

\begin{lstlisting}[language = python , frame = trBL , firstnumber = last , escapeinside={(*@}{@*)}]
// Get the Java runtime
Runtime runtime = Runtime.getRuntime();
// Run the garbage collector
runtime.gc();
// Calculate the used memory
long memory = runtime.totalMemory() - runtime.freeMemory();

memory = bytesToMegabytes(memory);
\end{lstlisting}
Per receiver werd deze code toegevoegd in de methode die de messages ontvangt. Deze code achterhaalt het gebruikte geheugen. De hulpmethode bytesToMegabytes(memory) zet het geheugen om naar megabytes. Dit resultaat werd dan ook meegegeven bij het aanmaken van een object van een entity en zo mee opgeslagen. Dit wil zeggen dat er in DataGPS, DataKafka en DataRMQ een attribuut memory bijkwam. Bij de resultaten van dit gedeelte van het onderzoek werd enkel rekening gehouden met het geheugen. Kijken naar het verschil tussen het verzenden en ontvangen in tijd was niet meer nodig. Het ophalen van het gebruik van geheugen neemt namelijk zo veel tijd in beslag dat het tijdsverschil niet meer relevant was om te bekijken.

Door dit nieuwe gedeelte van het onderzoek kwamen er 3 CSV bestanden bij. Eén bestand per technologie die het geheugen berekent. Er werd gekozen om telkens maar 1000 objecten te verzenden. Er kwamen dus ook drie nieuwe Python-scripts bij. Ook één per technologie die het gemiddelde, maximum en minimum van het geheugen berekende per technologie. Het opbouwen van deze scripts is gelijkaardig met het berekenen van het gemiddelde tijdsverschil. In plaats van het tijdsverschil werd enkel de kolom met het geheugengebruik bijgehouden.  